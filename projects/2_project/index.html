<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> üß† üîê 2. Trial Decoding | üß†üè∑Ô∏è Selection of experience for memory by hippocampal sharp wave ripples </title> <meta name="author" content="Wannan (Winnie) Yang"> <meta name="description" content="Demo codes to decode event identity with 4 different methods."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/RippleTagging/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/RippleTagging/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/RippleTagging/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/RippleTagging/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0%F0%9F%8F%B7%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/RippleTagging/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://winnieyangwannan.github.io/RippleTagging/projects/2_project/"> <script src="/RippleTagging/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> <link defer rel="stylesheet" href="/RippleTagging/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/RippleTagging//"> üß†üè∑Ô∏è Selection of experience for memory by hippocampal sharp wave ripples </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/RippleTagging/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/RippleTagging/Paper/">Paper </a> </li> <li class="nav-item "> <a class="nav-link" href="/RippleTagging/Results%20and%20Demos/">Results and Demos </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">üß† üîê 2. Trial Decoding</h1> <p class="post-description">Demo codes to decode event identity with 4 different methods.</p> </header> <article> <p>The companion notebook will walk you through the steps of trial identity decoding form spiking data!</p> <p>üîó Follow the demo code in Colab Notebook <a href="https://colab.research.google.com/drive/1D3BdR_TwraXgx7FEAUYx9MZkX5QEWjb1?usp=drive_link" rel="external nofollow noopener" target="_blank">here</a>! You will learn how to decode trial identity from population activity using four different methods!</p> <hr> <h1 id="part-1-how-does-umap-work">Part 1: How does UMAP work?</h1> <p>UMAP is an algorithm for dimension reduction based on manifold learning techniques and ideas from topological data analysis.</p> <p>At a very high level, UMAP is comprised of two main steps:</p> <ul> <li>Step 1: compute a graph representing your data in the high dimensional space (Fig. 1A). <ul> <li>In some sense in the end we have simply constructed a <strong>weighted graph</strong> where the weight is an aproxy of the distance between two data points in the high-dimensional space. Fig. 1 was designed to help us gain an intuitive understanding of this. As illustrated in Fig.1A, if two points are close to each other, the weight connecting the two are bigger, which corresponds to a thicker edge connecting the dots in Fig.1A.</li> <li>If you want to dig deeper into the theory, you can read more about the topological data analysis and simplicial complex theory behind this step <a href="https://umap-learn.readthedocs.io/en/latest/how_umap_works.html" rel="external nofollow noopener" target="_blank">here</a>, but at the implementational level, the Nearest-Neighbor-Descent algorithm of <a href="https://www.cs.princeton.edu/cass/papers/www11.pdf" rel="external nofollow noopener" target="_blank">Dong et al.</a> was implemented.</li> </ul> </li> <li>Step 2: learn a low-dimensional embedding that preserves the structure of the graph as much as possible (Fig.1B). <ul> <li>This step is essentially solving an <strong>optimization problem</strong>. Stochastic gradient descent was applied for the optimization process. The <strong>negative sampling</strong> trick (as used by <a href="https://arxiv.org/abs/1310.4546" rel="external nofollow noopener" target="_blank">word2vec</a>) was used to speed up the computation.</li> </ul> </li> </ul> <p>I recommend watching this <a href="https://www.youtube.com/watch?v=nq6iPZVUxZU&amp;t=913s&amp;ab_channel=Enthought" rel="external nofollow noopener" target="_blank">talk</a> if you are looking for an in-depth overview of UMAP. If you are looking for the mathematical description, please see the <a href="https://arxiv.org/abs/1802.03426" rel="external nofollow noopener" target="_blank">UMAP paper</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/RippleTagging/assets/img/demo/2/umap-480.webp 480w,/RippleTagging/assets/img/demo/2/umap-800.webp 800w,/RippleTagging/assets/img/demo/2/umap-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/RippleTagging/assets/img/demo/2/umap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> </div> </div> <div class="caption"> Fig 1. Two main steps of the UMAP dimensionality reduction process. </div> <hr> <h1 id="part-2-supervised-dimensionality-reduction">Part 2: Supervised dimensionality reduction</h1> <p>In order to better visualize the trial-by-trial representation drift and check if the progression of population representation drift aligns with the order of the events, we took advantage of the <a href="https://umap-learn.readthedocs.io/en/latest/supervised.html" rel="external nofollow noopener" target="_blank">supervised dimensionality reduction feature of the UMAP</a>.</p> <p>For unsupervised dimensionality reduction,the only input to the UMAP algorithm is the spiking data. For supervised dimensionality reduction, we also pass the class membership (in this case trial identity) as input. We can simply pass the UMAP model the trial label with the following code when fitting , and UMAP will make use of it to perform supervised dimension reduction! It is very easy!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embedding</span> <span class="o">=</span> <span class="n">umap</span><span class="p">.</span><span class="nc">UMAP</span><span class="p">().</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">trial_label</span><span class="p">)</span>
</code></pre></div></div> <p>How does UMAP make use of the trial label information under the hood? When optimizing the low-dimensional embedding, UMAP will pack the data with the same class label closer to each other while push the data with different label far away from each other.</p> <p>As described in <a href="https://winnieyangwannan.github.io/RippleTagging/projects/1_project/">the first demo</a>, we record from many hundreds of neurons simultaneously in the dorsal CA1 region of the hippocampus while the animal perform the figure-8 maze task. We plotted the result of supervised dimensionality reduction with UMAP as interactive plots below. We first colored the manifold with linearized position of the mouse (Fig. 2) and then colored it with trial block number (Fig. 3). The left panel is the neural manifold (the low-dimensional representation of the spiking data), and the right panel is the corresponding behavior trajectory of the animal.</p> <p>If you zoom in to the neural manifold plot, you will see that the manifold is actually comprised of many tiny dots. Each dot correspond to the low-dimensional representation of a population spiking vector (spike count of a 100-ms bin).</p> <div class="l-page"> <iframe src="/RippleTagging/assets/html/demo/umap_supervised_pos.html" frameborder="0" scrolling="no" height="500px" width="1000px" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Fig 2. An <mark>interactive</mark> plot showing the supervised UMAP embedding of the neural data when a mouse was running on the figure-8 maze. (Left) UMAP embedding of population activity. Each point corresponds to the low-dimensional representation of one binned spiking data. (Right) running trajectory of a mouse on the figure-8 maze. Both the neural manifold and the running trajectory were colored by the animal's linearized position. </div> <blockquote class="block-tip"> <h5 id="tip">TIP!</h5> <p>This is an interactive plot! You can rotate the manifold and examine the manifold from different angles!! This figure might take a few seconds to render! Try to rotate the interactive plot such that the manifolds for the early trials (blue ones) are at the bottom of the plot and the red manifolds are at the top.</p> </blockquote> <div class="l-page"> <iframe src="/RippleTagging/assets/html/demo/umap_supervised_trial.html" frameborder="0" scrolling="no" height="500px" width="1000px" style="border: 1px dashed grey;"></iframe> </div> <div class="caption"> Fig 3. Similar to the previous plot, this <mark>interactive</mark> plot shows the supervised UMAP embedding of the neural data when a mouse was running on the figure-8 maze.Both the neural manifold and the running trajectory were colored by trial block number. </div> <p>Comparing to the manifold from <a href="https://winnieyangwannan.github.io/RippleTagging/projects/1_project/">the unsupervised dimensionality reduction</a>. We can see that the manifold of different trial blocks are now clearly separated. Aside from the clear class separation however (which is expected ‚Äì we gave the algorithm all the class information), there are a couple of important points to note:</p> <ul> <li>The first point to note is that we have retained the internal structure of the individual classes. Notice that each manifold of the corresponding trial block preserves the topology of the maze. This is especially clear when you look at the manifold colored by the position of the animal (Fig. 2). The location at the beginning of the trial was colored in blue, the reward location at both sides of the arms are colored in green and the end of the trial at both arms are colored in red.</li> <li>The second point to note is that we have also retained the global structure. While the individual classes have been cleanly separated from one another, the inter-relationships among the classes have been preserved (Fig. 3)- The trial events that are closer together in time are also embedded closer together in the low-dimensional space. Notice that this is nontrivial!</li> </ul> <p>To summarize, supervised dimensionality reduction provides an intuitive visualization for the event specific representation in the hippocampus. We can appreciate that the representation drift is highly <strong>structured</strong>, progressing systematically in the same order as unfolding of the events in time! To better understand why this is nontrivial, one can imagine an alternative scenario where the representation of different events is still very different (we can see the different manifold are separated), but there is no <strong>structured</strong> relationship between the manifold ‚Äì the manifold for early trial is embedded very close to the late trial rather than far away from it.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/RippleTagging/assets/img/demo/2/shuffle-480.webp 480w,/RippleTagging/assets/img/demo/2/shuffle-800.webp 800w,/RippleTagging/assets/img/demo/2/shuffle-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/RippleTagging/assets/img/demo/2/shuffle.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> </div> </div> <div class="caption"> Fig 4. Supervised dimensionality reduction for shuffled data. </div> <p>So far, the visualizations have provided us valuable intuition of the nature of the population representation that is hard to grasp or imagine otherwise. Now we want to go beyond visualization, and to quantify what we have observed! One way to quantify what we see is through decoding ‚Äì if we believe that the hippocampus not only represent space but also the identity of the event, we are expecting to be able to decode the trial identity accurately from the hippocampal population activity. This is what we are going to do in the next part!</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <iframe width="677" height="500" src="https://www.youtube.com/embed/BuAeQmzt5Lc" title="UMAP manifold (supervised) for figure-8 maze task." frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> </div> <div class="caption"> </div> <hr> <h1 id="part-3-trial-decoding">Part 3: Trial decoding</h1> <p>To quantify the trial sequence information present in the state space, we tested if trial block membership can be accurately decoded from the population activity.</p> <p>Here we give a very high level overview of the main steps of the decoding procedure (Fig. 5):</p> <ul> <li>Step 1- 2 is just the dimensionality reduction process. We have described Step 1 and 2 in part 1 of this post when introducing how UMAP works.</li> <li>Step 3 is the actual decoding step. Suppose that we are going to decode the trial identity of an example data point (purple triangle in Fig. 5C). We can simply use the k-nearest neighbor (KNN) algorithm to find the k nearest neighbors of the purple triangle, and then take the mode of the trial label of those nearest neighbors as the trial label for our purple triangle.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/RippleTagging/assets/img/demo/2/decoding-480.webp 480w,/RippleTagging/assets/img/demo/2/decoding-800.webp 800w,/RippleTagging/assets/img/demo/2/decoding-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/RippleTagging/assets/img/demo/2/decoding.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> </div> </div> <div class="caption"> Fig 5. Main steps trial decoding using UMAP and kNN classifier. </div> <hr> <h1 id="part-4-cross-validation-with-leaving-one-trial-out">Part 4: Cross-validation with ‚Äòleaving one trial out‚Äô</h1> <p>Now that we have a way to perform decoding, we need to properly quantify the decoding result with cross-validation.</p> <p>For cross-validation, we used the standard 10-fold cross-validation as well as the more targeted ‚Äòleaving one trial out‚Äô method. We want to highlight the ‚Äòleaving one trial out‚Äô cross-validation procedure here because we believe it is the most convincing evidence that the population activity pattern varied systematically across trials.</p> <p>One important property of decoding is that as long as the different activity patterns of different trials are sufficiently differentiable, they can be decoded ‚Äì they don‚Äôt have to be structured to be decodable.What we want to demonstrate here is that not only the population activity between different events are different (and thus decodable), they are also highly structured. Thus. we need to find a method from which we can conclude not only decodability, but also demonstrate structured variation across trials!</p> <p>‚ÄòLeaving one trial out‚Äô is such a method (Fig. 6). To elaborate, ‚Äòleaving one trial out‚Äô is expected to yield high decoding accuracy only if the state space of the data changed in a structured way, evolving along one axis according to the sequence of trial events. This is because this validation method makes sure that the training set does not contain any data that shar the trial block membership with the test set, the test data could be decoded only to the next closest trial block (but not the same trial block) in the state space (Fi6. 5C). Note that this is why the diagonal (red dashed line) of the confusion matrix had 0 decoding probability ‚Äì the training and test data did not share any data from the same trial block and so cannot be decoded to the same trial block. If the neural embedding is structured according to the progression of trial events in time, the test data will be correctly decoded to their nearest neighbors and occupy the space immediately next to the diagonal of the confusion matrix.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/RippleTagging/assets/img/demo/2/validation_methods_github-480.webp 480w,/RippleTagging/assets/img/demo/2/validation_methods_github-800.webp 800w,/RippleTagging/assets/img/demo/2/validation_methods_github-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/RippleTagging/assets/img/demo/2/validation_methods_github.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Fig 6. Leave entire trial block out validation results from four different decoding methods. </div> <details><summary>Click here to read the full figure legend.</summary> <p>(A) Illustration of the hold entire trial block out validation method. Top, neural manifold from all the data in one example session. To give an example of the cross-validation procedure, data from one trial block (trial block 4; blue) was highlighted. Middle, the highlighted trial block in A was held out as test data. After removing the test data, a kNN decoder was trained only using training data (grey). Bottom, the test data was then embedded with the learned manifold (generated from training data), and the kNN decoder was used for trial block membership decoding. Test data was decoded to the trial block label of its nearest neighbor (trial block 3) on the training data manifold. Training data was gray and test data was colored by the identity of the decoded trial block.</p> <p>(B) Illustration of the 10-fold cross-validation method. Top, neural manifold generated from all the data in one example session. 10% of the data were highlighted (colored by their true trial block identity). The highlighted data was held out as test data. Middle, after removing the t est data, a kNN decoder was trained only using training data (grey). Bottom, embedding the test data with the training manifold and us ing the kNN decoder for trial block identity decoding. Training data was gray and test data was colored by the identity of the decoded trial block (trial block 3).</p> <p>(C) Confusion matrix obtained using hold entire trial block out validation. Note that the diagonal (red dashed line) of the confusion matrix had 0 decoding probability because the training and test data did not share any data from the same trial block. Since the neural embedding was structured according to the progression of trial events, the test data were correctly decoded to their nearest neighbors.</p> <p>(D) Confusion matrix from UMAP trial block decoding using hold entire trial block out validation.</p> </details> <hr> <h1 id="part-5-decoding-from-the-original-high-dimensional-space-and-pca-space">Part 5: Decoding from the original high-dimensional space and PCA space</h1> <p>Can we trust the decoding result from the UMAP embedding? We know that dimensionality reduction can potentially distort the relationship between data points in the original dimensional space. In order to make sure our conclusion is not the result of potential distortion during the UMAP dimensionality reduction process, we decoded from the original high-dimensional space (Fig.6 B and C) as well as PCA space (Fig.7 A).</p> <p>We can decode from the PCA space in an analogous way as decoding from the low dimensional embedding from UMAP. Our <a href="https://winnieyangwannan.github.io/RippleTagging/projects/1_project/">previous post</a> showed that PCA does not generate good visualization comparing to UMAP (Fig. 7 A and B). However, that does not mean that we cannot decode from the PCA space! As we increase the number of principal components in the low-dimensional space, we can see that the decoding error actually decrease (Fig.7 D). If we use the first 15 principal components (decoding from a 15 dimensional space), the decoding accuracy is already comparable with decoding from the UMAP space (Fig.7 C and E).</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/RippleTagging/assets/img/demo/2/pca-480.webp 480w,/RippleTagging/assets/img/demo/2/pca-800.webp 800w,/RippleTagging/assets/img/demo/2/pca-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/RippleTagging/assets/img/demo/2/pca.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Fig 7. Comparing PCA and UMAP. </div> <p>Fig. 8A illustrates the main steps of decoding from the original high dimensional space. As you can see, it follows the similar basic logic as decoding from the low-dimensional space (Fig.8 B). The only difference is that instead of performing kNN on the low-dimensional space, kNN was applied on the original high-dimensional space.</p> <p>Another point worth to note when decoding from the high dimensional space is that when finding the nearest neighbour for decoding, the notion of distance is inevitable. What is the correct distance metric to use? In our paper, we compared both Euclidean distance, as well as the cosine similarity as distance metric.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/RippleTagging/assets/img/demo/2/high_low-480.webp 480w,/RippleTagging/assets/img/demo/2/high_low-800.webp 800w,/RippleTagging/assets/img/demo/2/high_low-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/RippleTagging/assets/img/demo/2/high_low.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Fig 8. Comparing decoding from the original high-dimensional space and the low-dimensional space. </div> <details><summary>Click here to read the full figure legend.</summary> <p>(A) Diagram explaining the main steps of trial block decoding from the original high -dimensional space. Step 1: by default, a weighted nearest neighbor (kNN) graph, which connects each datapoint to its nearest neighbors, was constructed and used to generate the initial topological representation of the training dataset (dots in three different colors). Step 2: the trial block membership of the test data (purple triangle) was decoded by taking the mode of its k nearest neighbors. For example, the example test data point (purple triangle) was decoded to be trial block 3 because all of its three nearest neighbors in the original high-dimensional space belong to trial block 3 (purple).</p> <p>(B) Diagram explaining the main steps of trial block decoding from the low -dimensional space generated by UMAP. Step 1 was the same as in panel (A). Step 2: optimize the low -dimensional representation to preserve the topological representation as much as possible to that in the original high-dimensional space. Step 3: The trial block membership of the test data (purple triangle) was decoded by taking the mode of its nearest neighbors in the low -dimensional embedding. For example, the example test data point (purple triangle) was decoded to be trial block 3 because all its three nearest neighbors in the low -dimensional space belong to trial 3 block (purple).</p> </details> <p>As we can see, all those methods yield consistent conclusion with UMAP (Fig 9).</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/RippleTagging/assets/img/demo/2/validation_methods_other_github-480.webp 480w,/RippleTagging/assets/img/demo/2/validation_methods_other_github-800.webp 800w,/RippleTagging/assets/img/demo/2/validation_methods_other_github-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/RippleTagging/assets/img/demo/2/validation_methods_other_github.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Fig 9. Confusion matrix of trial block decoding results. </div> <hr> <h1 id="part-6-can-we-use-umap-for-trial-decoding">Part 6: Can we use UMAP for trial decoding?</h1> <p>UMAP can potentially introduce distortion during the dimensionality reduction process, and those distortions could affect the accuracy of the trial decoding results. In oder to demonstrate that we could use UMAP for trial decoding, we need to demonstrate that the trial block membership is consistent between the high and low-dimensional space.</p> <p>Recall that we search for the k nearest neighbor of the test data and use their trial label as the trial label for the test data. This procedure can be applied to both the high-dimensional space as well the low-dimensional space. In order to test if UMAP introduce any distortion for trial membership decoding, for each data point, we can simply compute the consistency of the trial membership of the k nearest neighbours in the high-dimensional space and the low-dimensional UMAP embedding! If the trial membership the neighbours are consistent, then this means the UMAP dimensionality reduction process does not introduce distortion that affect accuracy of the trial membership decoding.</p> <p>As we can see from Fig.10, the trial membership of the first 4 nearest neighbours between the high and low-dimensional space is 100% consistent! Note that this is not to say that UMAP perfectly preserve every little detail in the original high-dimensional space, rather this is to say that potential distortion that occurs at the dimensionality reduction process does not affect the trial decoding membership. Thus, we can use UMAP to quantify trial decoding.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/RippleTagging/assets/img/demo/2/consistent-480.webp 480w,/RippleTagging/assets/img/demo/2/consistent-800.webp 800w,/RippleTagging/assets/img/demo/2/consistent-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/RippleTagging/assets/img/demo/2/consistent.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> </div> </div> <div class="caption"> Fig 10. Consistent trial block membership between the low-dimensional and the original high-dimensional space. </div> <details><summary>Click here to read the full figure legend.</summary> <p>To check whether the UMAP dimensionality reduction process introduced distortions that affected the decoded trial block membership (5-trial block), the consistency between the original high- dimensional space and reduced low- dimensional space was plotted. For each data point, the decoded trial block membership of its ten nea rest neighbors in the high-dimensional space and the reduced low-dimensional space were compared. A data point was considered as having consistent trial block membership if trial_high - trial_low = 0 (i.e. being decoded to the same trial block). For the first four nearest neighbors , the decoded trial block membership between the high and low-dimensional space was 100% consistent. Thus, UMAP dimensionality reduction did not introduce any distortion at the level of trial block decoding results.</p> </details> <hr> <h1 id="part-7-can-electrode-drift-explain-what-we-see">Part 7: Can electrode drift explain what we see?</h1> <p>Another important issue we need to properly address is recording stability. We need to exclude the possibility that the representation drift is not an artifact of electrode drift.</p> <p>To this end, we plotted the amplitude of waveform over time as well as compared the spike amplitude in the first quarter and the last quarter of the session (Fig. 11). In addition, we also quantified waveform centroid displacement across time following <a href="https://pubmed.ncbi.nlm.nih.gov/34108681/" rel="external nofollow noopener" target="_blank">Schoonover et al.</a>.</p> <p>We also did additional comprehensive analysis to demonstrate that excluding the most unstable units does not change our conclusion. For readers interested in this, you can refer to our <a href="">paper</a> (Fig. S9) for more details.</p> <p>Our analyses indicate that electrode drift cannot explain the representational drift.</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset="/RippleTagging/assets/img/demo/2/stationary-480.webp 480w,/RippleTagging/assets/img/demo/2/stationary-800.webp 800w,/RippleTagging/assets/img/demo/2/stationary-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/RippleTagging/assets/img/demo/2/stationary.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Fig 11. Assessment of single-unit stability over time. </div> <details><summary>Click here to read the full figure legend.</summary> <p>(A) Waveform amplitude over time (on-maze duration) for all the cells in an example session. Spikes of individual neurons are color-coded by amplitude. The spike amplitude value was extracted from Amplitudes.npy, which was generated by Kilosort/Phy. It is a measure of each spike‚Äôs amplitude in the template space.</p> <p>(B) Scatter plot of waveform amplitude in the first quarter of on- maze recording compared to the last quarter , for all the isolated units from the entire figure-8 maze dataset (Pearson correlation coefficient, N = 4469 cells from 6 animals).</p> <p>(C) Single-unit waveform centroid displacement across time, from a representative session , plotted in 3-D space. Centroid was computed by taking the spatial average across electrode positions weighted by the squared mean waveform amplitude at each electrode (see Methods for details). Waveform centroid for each single unit during the first quarter (blue circles) and last quarter of recording (red triangles) were connected by red lines (note: most of them were very short and not visible). The large semi-transparent orange and blue circles indicate the positions of the dual-sided probe‚Äôs 128 electrode sites (blue, and orange, front and back sides of the dual-sided probe, respectively).</p> <p>(D) Cumulative distribution of unit centroid displacement between the first and the last quarter of maze recording for all the cells in the dataset (median = 2.81- um, Q1 = 1.37- um, Q3 = 5.46- um; N= 4469 cells from 6 animals).</p> </details> <hr> <h1 id="how-to-cite-us-">How to cite us ?</h1> <p>Enjoyed reading this post and found our demo code useful? It can be cited as follows:</p> <p>Wannan Yang, Chen Sun, Roman Husz√°r, Thomas Hainmueller, Kirill Kiselev, Gy√∂rgy Buzs√°ki. ‚ÄúSelection of experience for memory by hippocampal sharp wave ripple.‚Äù <em>Science</em> (2024).</p> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2024 Wannan (Winnie) Yang. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/RippleTagging/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/RippleTagging/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/RippleTagging/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/RippleTagging/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/RippleTagging/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/RippleTagging/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/RippleTagging/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/RippleTagging/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>